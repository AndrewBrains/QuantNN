{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DuringTrain.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X3d6boFVn14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install larq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVBSETCoW9ie",
        "colab_type": "code",
        "outputId": "ac073bbe-c6bb-4a96-8756-0d17b7f7063b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import larq as lq\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import numpy as np\n",
        "\n",
        "# learning rate tuning during learning\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    lrate = 0.01\n",
        "    if epoch > 25:\n",
        "        lrate = 0.001\n",
        "    if epoch > 50:\n",
        "        lrate = 0.0005        \n",
        "    return lrate\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "#z-score\n",
        "mean = np.mean(x_train,axis=(0,1,2,3))\n",
        "std = np.std(x_train,axis=(0,1,2,3))\n",
        "x_train = (x_train-mean)/(std+1e-7)\n",
        "x_test = (x_test-mean)/(std+1e-7)\n",
        "\n",
        "num_classes = 10\n",
        "y_train = np_utils.to_categorical(y_train,num_classes)\n",
        "y_test = np_utils.to_categorical(y_test,num_classes)\n",
        "\n",
        "weight_decay = 1e-4\n",
        "\n",
        "\n",
        "kwargs = dict(input_quantizer=\"ste_sign\",\n",
        "              kernel_quantizer=\"ste_sign\",\n",
        "              kernel_constraint=\"weight_clip\"\n",
        "              )\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    # In the first layer we only quantize the weights and not the input\n",
        "    lq.layers.QuantConv2D(32, (3,3),\n",
        "                          padding=\"same\",\n",
        "                          kernel_quantizer=\"ste_sign\",\n",
        "                          kernel_constraint=\"weight_clip\",\n",
        "                          activation='relu',\n",
        "                          input_shape=x_train.shape[1:]),\n",
        "    #tf.keras.layers.Activation('relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    lq.layers.QuantConv2D(32, (3,3), padding=\"same\", activation='relu',**kwargs),\n",
        "    #tf.keras.layers.Activation('relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
        "    lq.layers.QuantConv2D(64, (3,3), padding=\"same\", activation='relu',**kwargs),\n",
        "    #tf.keras.layers.Activation('relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    lq.layers.QuantConv2D(64, (3,3), padding=\"same\", activation='relu',**kwargs),\n",
        "    #tf.keras.layers.Activation('relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
        "    lq.layers.QuantConv2D(128, (3,3), padding=\"same\", activation='relu',**kwargs),\n",
        "    #tf.keras.layers.Activation('relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    lq.layers.QuantConv2D(128, (3,3), padding=\"same\", activation='relu',**kwargs),\n",
        "    #tf.keras.layers.Activation('relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    lq.layers.QuantDense(10, **kwargs,activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    )\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# training\n",
        "batch_size = 64\n",
        "\n",
        "opt_rms = tf.keras.optimizers.RMSprop(lr=0.001,decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
        "\n",
        "lq.models.summary(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+sequential_10 stats----------------------------------------------------------------------------------------+\n",
            "| Layer                   Input prec.           Outputs  # 1-bit  # 32-bit  Memory  1-bit MACs  32-bit MACs |\n",
            "|                               (bit)                        x 1       x 1    (kB)                          |\n",
            "+-----------------------------------------------------------------------------------------------------------+\n",
            "| quant_conv2d_60                   -  (-1, 32, 32, 32)      864        32    0.23           0       884736 |\n",
            "| batch_normalization_66            -  (-1, 32, 32, 32)        0        64    0.25           0            0 |\n",
            "| quant_conv2d_61                   1  (-1, 32, 32, 32)     9216        32    1.25     9437184            0 |\n",
            "| batch_normalization_67            -  (-1, 32, 32, 32)        0        64    0.25           0            0 |\n",
            "| max_pooling2d_30                  -  (-1, 16, 16, 32)        0         0       0           0            0 |\n",
            "| quant_conv2d_62                   1  (-1, 16, 16, 64)    18432        64    2.50     4718592            0 |\n",
            "| batch_normalization_68            -  (-1, 16, 16, 64)        0       128    0.50           0            0 |\n",
            "| quant_conv2d_63                   1  (-1, 16, 16, 64)    36864        64    4.75     9437184            0 |\n",
            "| batch_normalization_69            -  (-1, 16, 16, 64)        0       128    0.50           0            0 |\n",
            "| max_pooling2d_31                  -    (-1, 8, 8, 64)        0         0       0           0            0 |\n",
            "| quant_conv2d_64                   1   (-1, 8, 8, 128)    73728       128    9.50     4718592            0 |\n",
            "| batch_normalization_70            -   (-1, 8, 8, 128)        0       256    1.00           0            0 |\n",
            "| quant_conv2d_65                   1   (-1, 8, 8, 128)   147456       128   18.50     9437184            0 |\n",
            "| batch_normalization_71            -   (-1, 8, 8, 128)        0       256    1.00           0            0 |\n",
            "| max_pooling2d_32                  -   (-1, 4, 4, 128)        0         0       0           0            0 |\n",
            "| flatten_10                        -        (-1, 2048)        0         0       0           0            0 |\n",
            "| quant_dense_14                    1          (-1, 10)    20480        10    2.54       20480            0 |\n",
            "+-----------------------------------------------------------------------------------------------------------+\n",
            "| Total                                                   307040      1354   42.77    37769216       884736 |\n",
            "+-----------------------------------------------------------------------------------------------------------+\n",
            "+sequential_10 summary------------------------+\n",
            "| Total params                      308 k     |\n",
            "| Trainable params                  307 k     |\n",
            "| Non-trainable params              896       |\n",
            "| Model size                        42.77 KiB |\n",
            "| Model size (8-bit FP weights)     38.80 KiB |\n",
            "| Float-32 Equivalent               1.18 MiB  |\n",
            "| Compression Ratio of Memory       0.04      |\n",
            "| Number of MACs                    38.7 M    |\n",
            "| Ratio of MACs that are binarized  0.9771    |\n",
            "+---------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCMOMwDBWSFy",
        "colab_type": "code",
        "outputId": "af6a481e-8c55-4008-e2fb-be32f7085ca4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=75,\n",
        "                    verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 138.2786 - accuracy: 0.1950 - val_loss: 72.7854 - val_accuracy: 0.2713 - lr: 0.0100\n",
            "Epoch 2/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 90.6262 - accuracy: 0.2574 - val_loss: 96.1414 - val_accuracy: 0.3090 - lr: 0.0100\n",
            "Epoch 3/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 71.7008 - accuracy: 0.3015 - val_loss: 60.0726 - val_accuracy: 0.2988 - lr: 0.0100\n",
            "Epoch 4/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 60.5547 - accuracy: 0.3223 - val_loss: 66.1529 - val_accuracy: 0.2686 - lr: 0.0100\n",
            "Epoch 5/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 50.6748 - accuracy: 0.3446 - val_loss: 54.0107 - val_accuracy: 0.3230 - lr: 0.0100\n",
            "Epoch 6/75\n",
            "781/781 [==============================] - 46s 60ms/step - loss: 44.5796 - accuracy: 0.3613 - val_loss: 31.7182 - val_accuracy: 0.4131 - lr: 0.0100\n",
            "Epoch 7/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 40.7113 - accuracy: 0.3746 - val_loss: 38.4288 - val_accuracy: 0.3716 - lr: 0.0100\n",
            "Epoch 8/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 38.9288 - accuracy: 0.3785 - val_loss: 57.5024 - val_accuracy: 0.2830 - lr: 0.0100\n",
            "Epoch 9/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 35.4259 - accuracy: 0.4007 - val_loss: 42.9237 - val_accuracy: 0.3731 - lr: 0.0100\n",
            "Epoch 10/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 33.9143 - accuracy: 0.4045 - val_loss: 57.5932 - val_accuracy: 0.3691 - lr: 0.0100\n",
            "Epoch 11/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 33.1694 - accuracy: 0.4174 - val_loss: 30.2127 - val_accuracy: 0.4436 - lr: 0.0100\n",
            "Epoch 12/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 31.5530 - accuracy: 0.4255 - val_loss: 33.2812 - val_accuracy: 0.4565 - lr: 0.0100\n",
            "Epoch 13/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 30.5963 - accuracy: 0.4245 - val_loss: 23.3569 - val_accuracy: 0.4954 - lr: 0.0100\n",
            "Epoch 14/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 31.1692 - accuracy: 0.4269 - val_loss: 28.3017 - val_accuracy: 0.4292 - lr: 0.0100\n",
            "Epoch 15/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 33.8137 - accuracy: 0.4308 - val_loss: 32.5984 - val_accuracy: 0.4567 - lr: 0.0100\n",
            "Epoch 16/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 36.9272 - accuracy: 0.4395 - val_loss: 53.1628 - val_accuracy: 0.3572 - lr: 0.0100\n",
            "Epoch 17/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 33.1313 - accuracy: 0.4501 - val_loss: 32.0117 - val_accuracy: 0.5187 - lr: 0.0100\n",
            "Epoch 18/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 33.3650 - accuracy: 0.4553 - val_loss: 29.5417 - val_accuracy: 0.4183 - lr: 0.0100\n",
            "Epoch 19/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 28.6086 - accuracy: 0.4704 - val_loss: 37.6046 - val_accuracy: 0.4039 - lr: 0.0100\n",
            "Epoch 20/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 25.9773 - accuracy: 0.4780 - val_loss: 29.6343 - val_accuracy: 0.4510 - lr: 0.0100\n",
            "Epoch 21/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 25.7800 - accuracy: 0.4799 - val_loss: 51.5915 - val_accuracy: 0.3534 - lr: 0.0100\n",
            "Epoch 22/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 24.4210 - accuracy: 0.4843 - val_loss: 35.5660 - val_accuracy: 0.4137 - lr: 0.0100\n",
            "Epoch 23/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 25.2308 - accuracy: 0.4802 - val_loss: 22.6736 - val_accuracy: 0.5071 - lr: 0.0100\n",
            "Epoch 24/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 24.6492 - accuracy: 0.4820 - val_loss: 14.0465 - val_accuracy: 0.5701 - lr: 0.0100\n",
            "Epoch 25/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 23.6242 - accuracy: 0.4885 - val_loss: 28.1252 - val_accuracy: 0.4872 - lr: 0.0100\n",
            "Epoch 26/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 23.0894 - accuracy: 0.4946 - val_loss: 23.9072 - val_accuracy: 0.4654 - lr: 0.0100\n",
            "Epoch 27/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 9.4937 - accuracy: 0.6071 - val_loss: 9.6625 - val_accuracy: 0.6383 - lr: 0.0010\n",
            "Epoch 28/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 8.7920 - accuracy: 0.5943 - val_loss: 9.2815 - val_accuracy: 0.6067 - lr: 0.0010\n",
            "Epoch 29/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 9.3131 - accuracy: 0.5800 - val_loss: 10.1125 - val_accuracy: 0.5581 - lr: 0.0010\n",
            "Epoch 30/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 9.4785 - accuracy: 0.5736 - val_loss: 9.8093 - val_accuracy: 0.6008 - lr: 0.0010\n",
            "Epoch 31/75\n",
            "781/781 [==============================] - 46s 60ms/step - loss: 9.6151 - accuracy: 0.5728 - val_loss: 8.2938 - val_accuracy: 0.6038 - lr: 0.0010\n",
            "Epoch 32/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 10.6449 - accuracy: 0.5617 - val_loss: 12.7841 - val_accuracy: 0.5526 - lr: 0.0010\n",
            "Epoch 33/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 10.9933 - accuracy: 0.5633 - val_loss: 12.1920 - val_accuracy: 0.5628 - lr: 0.0010\n",
            "Epoch 34/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 11.4781 - accuracy: 0.5575 - val_loss: 8.4293 - val_accuracy: 0.6387 - lr: 0.0010\n",
            "Epoch 35/75\n",
            "781/781 [==============================] - 47s 61ms/step - loss: 11.1807 - accuracy: 0.5599 - val_loss: 16.9307 - val_accuracy: 0.4616 - lr: 0.0010\n",
            "Epoch 36/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 11.8964 - accuracy: 0.5528 - val_loss: 14.1106 - val_accuracy: 0.5346 - lr: 0.0010\n",
            "Epoch 37/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 10.9828 - accuracy: 0.5572 - val_loss: 16.4839 - val_accuracy: 0.4697 - lr: 0.0010\n",
            "Epoch 38/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 11.4283 - accuracy: 0.5575 - val_loss: 19.2087 - val_accuracy: 0.4366 - lr: 0.0010\n",
            "Epoch 39/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 12.3359 - accuracy: 0.5538 - val_loss: 10.3902 - val_accuracy: 0.5724 - lr: 0.0010\n",
            "Epoch 40/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 11.7581 - accuracy: 0.5519 - val_loss: 9.8398 - val_accuracy: 0.6167 - lr: 0.0010\n",
            "Epoch 41/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 14.4888 - accuracy: 0.5387 - val_loss: 12.0183 - val_accuracy: 0.5928 - lr: 0.0010\n",
            "Epoch 42/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 14.9499 - accuracy: 0.5431 - val_loss: 17.5563 - val_accuracy: 0.5633 - lr: 0.0010\n",
            "Epoch 43/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 14.7801 - accuracy: 0.5473 - val_loss: 12.4025 - val_accuracy: 0.6136 - lr: 0.0010\n",
            "Epoch 44/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 13.4174 - accuracy: 0.5550 - val_loss: 13.4391 - val_accuracy: 0.5581 - lr: 0.0010\n",
            "Epoch 45/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 12.7197 - accuracy: 0.5529 - val_loss: 17.4875 - val_accuracy: 0.5192 - lr: 0.0010\n",
            "Epoch 46/75\n",
            "781/781 [==============================] - 46s 60ms/step - loss: 14.1752 - accuracy: 0.5481 - val_loss: 14.3116 - val_accuracy: 0.5633 - lr: 0.0010\n",
            "Epoch 47/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 13.6353 - accuracy: 0.5564 - val_loss: 14.7320 - val_accuracy: 0.5533 - lr: 0.0010\n",
            "Epoch 48/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 14.5525 - accuracy: 0.5463 - val_loss: 14.7212 - val_accuracy: 0.5341 - lr: 0.0010\n",
            "Epoch 49/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 14.7160 - accuracy: 0.5503 - val_loss: 10.7481 - val_accuracy: 0.5979 - lr: 0.0010\n",
            "Epoch 50/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 14.9044 - accuracy: 0.5429 - val_loss: 10.5057 - val_accuracy: 0.6097 - lr: 0.0010\n",
            "Epoch 51/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 15.3510 - accuracy: 0.5491 - val_loss: 12.7756 - val_accuracy: 0.5811 - lr: 0.0010\n",
            "Epoch 52/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 11.4665 - accuracy: 0.5719 - val_loss: 15.5600 - val_accuracy: 0.5393 - lr: 5.0000e-04\n",
            "Epoch 53/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 14.0984 - accuracy: 0.5511 - val_loss: 12.4065 - val_accuracy: 0.5761 - lr: 5.0000e-04\n",
            "Epoch 54/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 13.8273 - accuracy: 0.5552 - val_loss: 13.8615 - val_accuracy: 0.5391 - lr: 5.0000e-04\n",
            "Epoch 55/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 13.3410 - accuracy: 0.5593 - val_loss: 14.2174 - val_accuracy: 0.5558 - lr: 5.0000e-04\n",
            "Epoch 56/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 11.4550 - accuracy: 0.5668 - val_loss: 11.3212 - val_accuracy: 0.5774 - lr: 5.0000e-04\n",
            "Epoch 57/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 10.8841 - accuracy: 0.5647 - val_loss: 7.6694 - val_accuracy: 0.6363 - lr: 5.0000e-04\n",
            "Epoch 58/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 11.5129 - accuracy: 0.5626 - val_loss: 10.6062 - val_accuracy: 0.6221 - lr: 5.0000e-04\n",
            "Epoch 59/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 11.8933 - accuracy: 0.5611 - val_loss: 12.5091 - val_accuracy: 0.5552 - lr: 5.0000e-04\n",
            "Epoch 60/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 10.5711 - accuracy: 0.5645 - val_loss: 10.1799 - val_accuracy: 0.6058 - lr: 5.0000e-04\n",
            "Epoch 61/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 11.5550 - accuracy: 0.5576 - val_loss: 12.6163 - val_accuracy: 0.5992 - lr: 5.0000e-04\n",
            "Epoch 62/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 12.3231 - accuracy: 0.5512 - val_loss: 14.1693 - val_accuracy: 0.4990 - lr: 5.0000e-04\n",
            "Epoch 63/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 11.7682 - accuracy: 0.5520 - val_loss: 16.0813 - val_accuracy: 0.4915 - lr: 5.0000e-04\n",
            "Epoch 64/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 13.0100 - accuracy: 0.5544 - val_loss: 11.3659 - val_accuracy: 0.6162 - lr: 5.0000e-04\n",
            "Epoch 65/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 14.2520 - accuracy: 0.5473 - val_loss: 13.9731 - val_accuracy: 0.5909 - lr: 5.0000e-04\n",
            "Epoch 66/75\n",
            "781/781 [==============================] - 45s 58ms/step - loss: 13.3353 - accuracy: 0.5571 - val_loss: 16.7752 - val_accuracy: 0.5143 - lr: 5.0000e-04\n",
            "Epoch 67/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 13.2982 - accuracy: 0.5508 - val_loss: 14.3843 - val_accuracy: 0.5557 - lr: 5.0000e-04\n",
            "Epoch 68/75\n",
            "781/781 [==============================] - 45s 58ms/step - loss: 13.5804 - accuracy: 0.5509 - val_loss: 9.3322 - val_accuracy: 0.6131 - lr: 5.0000e-04\n",
            "Epoch 69/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 15.0962 - accuracy: 0.5455 - val_loss: 8.4472 - val_accuracy: 0.6498 - lr: 5.0000e-04\n",
            "Epoch 70/75\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 14.7115 - accuracy: 0.5465 - val_loss: 20.4649 - val_accuracy: 0.5406 - lr: 5.0000e-04\n",
            "Epoch 71/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 14.6024 - accuracy: 0.5519 - val_loss: 12.6250 - val_accuracy: 0.5957 - lr: 5.0000e-04\n",
            "Epoch 72/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 14.3927 - accuracy: 0.5525 - val_loss: 13.2903 - val_accuracy: 0.5699 - lr: 5.0000e-04\n",
            "Epoch 73/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 16.0204 - accuracy: 0.5415 - val_loss: 19.0288 - val_accuracy: 0.5311 - lr: 5.0000e-04\n",
            "Epoch 74/75\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 15.2259 - accuracy: 0.5516 - val_loss: 14.8159 - val_accuracy: 0.5806 - lr: 5.0000e-04\n",
            "Epoch 75/75\n",
            "781/781 [==============================] - 46s 58ms/step - loss: 14.5117 - accuracy: 0.5550 - val_loss: 13.2858 - val_accuracy: 0.5934 - lr: 5.0000e-04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1d3f069588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}