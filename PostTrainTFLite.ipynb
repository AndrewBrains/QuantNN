{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PostTrainTFLite.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uFSmCVE1awty"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX-CLDVL0FQj",
        "colab_type": "text"
      },
      "source": [
        "CIFAR - 10 model with 89% accuracy on 32-bit floats\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5NiBFpEKY5Y",
        "colab_type": "text"
      },
      "source": [
        "#Original model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L9fvmjp39Lv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "66c41f68-5e75-4ba1-c95d-56bed559a082"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF9woeBotblR",
        "colab_type": "code",
        "outputId": "20cc6f45-1ddb-46dc-ce22-c24c5857392d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import numpy as np\n",
        "\n",
        "# learning rate tuning during learning\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    lrate = 0.001\n",
        "    if epoch > 25:\n",
        "        lrate = 0.0005\n",
        "    if epoch > 50:\n",
        "        lrate = 0.0003        \n",
        "    return lrate\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "#z-score\n",
        "mean = np.mean(x_train,axis=(0,1,2,3))\n",
        "std = np.std(x_train,axis=(0,1,2,3))\n",
        "x_train = (x_train-mean)/(std+1e-7)\n",
        "x_test = (x_test-mean)/(std+1e-7)\n",
        "\n",
        "num_classes = 10\n",
        "y_train = np_utils.to_categorical(y_train,num_classes)\n",
        "y_test = np_utils.to_categorical(y_test,num_classes)\n",
        "\n",
        "weight_decay = 1e-4\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    )\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# training\n",
        "batch_size = 64\n",
        "\n",
        "opt_rms = tf.keras.optimizers.RMSprop(lr=0.001,decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 309,290\n",
            "Trainable params: 308,394\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 309,290\n",
            "Trainable params: 308,394\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDWcBTHu5xns",
        "colab_type": "text"
      },
      "source": [
        "Use this to load exsits weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBwTKS-D4xYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('/content/gdrive/My Drive/BNN_works/QCIFAR10/PostTrain/original_model_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKO0B1PVDCTY",
        "colab_type": "text"
      },
      "source": [
        "Or this to re-train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdhEt1lp1cSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=75,\n",
        "                    verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])\n",
        "\n",
        "# save to disk\n",
        "#model_json = model.to_json()\n",
        "#with open('model.json', 'w') as json_file:\n",
        "#    json_file.write(model_json)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-rknPg-8jh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reform y_test\n",
        "\n",
        "y_test = [np.argmax(i) for i in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFSmCVE1awty",
        "colab_type": "text"
      },
      "source": [
        "#Test code about how TFLite works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kh1k05PuIGH",
        "colab_type": "code",
        "outputId": "b34ad875-c075-403c-d63f-172bfe028188",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFgUBLgrY15H",
        "colab_type": "code",
        "outputId": "36211fe4-fb05-4650-d66b-f5731d08ddef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2908 - accuracy: 0.9149\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1401 - accuracy: 0.9587\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1056 - accuracy: 0.9681\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0855 - accuracy: 0.9732\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0743 - accuracy: 0.9768\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe51859ca20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lbsABoEkjtP",
        "colab_type": "code",
        "outputId": "ab9938ea-c7ad-4d94-f5f4-862ae5e2ea1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Agh2V5aKVbFE",
        "colab_type": "code",
        "outputId": "9955251e-536b-4c1e-9133-33fb74c6e218",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import pathlib\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "#========================== NO QUANTIZATION ===========================\n",
        "\n",
        "tflite_models_dir = pathlib.Path(\"/content/gdrive/My Drive/BNN_works/quant models\")\n",
        "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
        "tflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "print(\"1. no quant, size: \", tflite_model_file.write_bytes(tflite_model))\n",
        "\n",
        "#===== 8 - bit SOFT QUANTIZATION (maybe not all would be quantized)=====\n",
        "\n",
        "mnist_train, _ = tf.keras.datasets.mnist.load_data()\n",
        "images = tf.cast(mnist_train[0], tf.float32) / 255.0\n",
        "mnist_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\n",
        "\n",
        "def representative_data_gen():\n",
        "  for input_value in mnist_ds.take(100):\n",
        "    yield [input_value]\n",
        "\n",
        "converter.representative_dataset = representative_data_gen\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model_quant = converter.convert()\n",
        "tflite_model_quant_file = tflite_models_dir/\"mnist_model_quant.tflite\"\n",
        "print(\"2. 8 bit quant, size: \",tflite_model_quant_file.write_bytes(tflite_model_quant))\n",
        "\n",
        "#================== 8 - bit QUANTIZATION with input & output ====================\n",
        "\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "tflite_model_quant = converter.convert()\n",
        "tflite_model_quant_file = tflite_models_dir/\"mnist_model_quant_io.tflite\"\n",
        "print(\"3. add input/output quant 8-bit, size: \",tflite_model_quant_file.write_bytes(tflite_model_quant))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. no quant, size:  408436\n",
            "2. 8 bit quant, size:  104368\n",
            "3. add input/output quant 8-bit, size:  104368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHjf7vpH3VuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFuq1uk8R9jb",
        "colab_type": "text"
      },
      "source": [
        "Lets test original model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7B55vxTJThx",
        "colab_type": "code",
        "outputId": "2b4fa98d-817e-405f-b3da-914db4a3ef2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "interpreter_quant = tf.lite.Interpreter(model_path=str(tflite_model_quant_file))\n",
        "interpreter_quant.allocate_tensors()\n",
        "\n",
        "input_index_quant = interpreter_quant.get_input_details()[0][\"index\"]\n",
        "output_index_quant = interpreter_quant.get_output_details()[0][\"index\"]\n",
        "\n",
        "test_image = np.expand_dims(x_test[0], axis=0).astype(np.float32)\n",
        "\n",
        "input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "interpreter.set_tensor(input_index, test_image)\n",
        "interpreter.invoke()\n",
        "predictions = interpreter.get_tensor(output_index)\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "plt.imshow(x_test[0])\n",
        "template = \"True:{true}, predicted:{predict}\"\n",
        "_ = plt.title(template.format(true= str(y_test[0]),\n",
        "                              predict=str(np.argmax(predictions[0]))))\n",
        "plt.grid(False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZh-kca3SBTX",
        "colab_type": "text"
      },
      "source": [
        "And now the quantized model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKlUuRn2SGG2",
        "colab_type": "code",
        "outputId": "ed176c95-b638-4edd-f4c2-e1bacb5c56c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "input_index = interpreter_quant.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter_quant.get_output_details()[0][\"index\"]\n",
        "interpreter_quant.set_tensor(input_index, test_image)\n",
        "interpreter_quant.invoke()\n",
        "predictions = interpreter_quant.get_tensor(output_index)\n",
        "\n",
        "plt.imshow(x_test[0])\n",
        "template = \"True:{true}, predicted:{predict}\"\n",
        "_ = plt.title(template.format(true= str(y_test[0]),\n",
        "                              predict=str(np.argmax(predictions[0]))))\n",
        "plt.grid(False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJdvhk-RVJdc",
        "colab_type": "text"
      },
      "source": [
        "Function to evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCs3kc26VOaI",
        "colab_type": "code",
        "outputId": "006592fa-54a6-4a6f-c71a-319a35440217",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# A helper function to evaluate the TF Lite model using \"test\" dataset.\n",
        "def evaluate_model(interpreter):\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on every image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  for test_image in x_test:\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  accurate_count = 0\n",
        "  for index in range(len(prediction_digits)):\n",
        "    if prediction_digits[index] == y_test[index]:\n",
        "      accurate_count += 1\n",
        "  accuracy = accurate_count * 1.0 / len(prediction_digits)\n",
        "\n",
        "  return accuracy\n",
        "\n",
        "print(evaluate_model(interpreter))            # float -32\n",
        "print(evaluate_model(interpreter_quant))      # int - 8"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9784\n",
            "0.9778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zq9mnPqWvOF",
        "colab_type": "text"
      },
      "source": [
        "#Original model quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvNpLX_sWzgi",
        "colab_type": "code",
        "outputId": "c86b5f0f-91e8-49c3-e752-eeb28f0d1766",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import pathlib\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)                                     #\n",
        "tflite_models_dir = pathlib.Path(\"/content/gdrive/My Drive/BNN_works/QCIFAR10/PostTrain\")       # organize file space\n",
        "tflite_models_dir.mkdir(exist_ok=True, parents=True)                                            #\n",
        "\n",
        "#========================== NO QUANTIZATION ===========================\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)  \n",
        "no_quant_model_file = tflite_models_dir/\"no_quant.tflite\"\n",
        "no_quant_model = converter.convert()\n",
        "print(\"1. no quant, size: \", no_quant_model_file.write_bytes(no_quant_model))\n",
        "\n",
        "#====================== REDUCED FLOAT - 16 bit =========================\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)  \n",
        "redused_float_model_file = tflite_models_dir/\"redused_float.tflite\"\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types=[tf.float16]\n",
        "redused_float_model = converter.convert()\n",
        "print(\"2. redused float, size: \", redused_float_model_file.write_bytes(redused_float_model))\n",
        "\n",
        "#======================== HYBRID QUANT =================================\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)  \n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "hybrid_model = converter.convert()\n",
        "hybrid_model_file = tflite_models_dir/\"hybrid_quant.tflite\"\n",
        "print(\"3. hybrid quant, size: \",hybrid_model_file.write_bytes(hybrid_model))\n",
        "\n",
        "#================== 8 - bit QUANTIZATION ====================\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)  \n",
        "cif_train, _ = tf.keras.datasets.cifar10.load_data()\n",
        "images = tf.cast(cif_train[0], tf.float32) / 255.0\n",
        "cif_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\n",
        "\n",
        "def representative_data_gen():\n",
        "  for input_value in cif_ds.take(100):\n",
        "    yield [input_value]\n",
        "\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "\n",
        "\n",
        "quant_model = converter.convert()\n",
        "quant_model_file = tflite_models_dir/\"integer_quant.tflite\"\n",
        "print(\"4. integer quant, size \",tflite_model_quant_file.write_bytes(quant_model))\n",
        "\n",
        "\n",
        "#converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "#tflite_model_quant = converter.convert()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. no quant, size:  1240216\n",
            "2. redused float, size:  626620\n",
            "3. hybrid quant, size:  321992\n",
            "4. integer quant, size  334048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hFpIQ6GbbC-",
        "colab_type": "code",
        "outputId": "0a529411-d156-4728-e557-a93289676b8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "interpreter_1 = tf.lite.Interpreter(model_path=str(no_quant_model_file))\n",
        "interpreter_1.allocate_tensors()\n",
        "\n",
        "interpreter_2 = tf.lite.Interpreter(model_path=str(redused_float_model_file))\n",
        "interpreter_2.allocate_tensors()\n",
        "\n",
        "interpreter_3 = tf.lite.Interpreter(model_path=str(hybrid_model_file))\n",
        "interpreter_3.allocate_tensors()\n",
        "\n",
        "interpreter_4 = tf.lite.Interpreter(model_path=str(quant_model_file))\n",
        "interpreter_4.allocate_tensors()\n",
        "\n",
        "\n",
        "'''input_index_quant = interpreter_quant.get_input_details()[0][\"index\"]\n",
        "output_index_quant = interpreter_quant.get_output_details()[0][\"index\"]\n",
        "\n",
        "test_image = np.expand_dims(x_test[1], axis=0).astype(np.float32)\n",
        "\n",
        "input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "interpreter.set_tensor(input_index, test_image)\n",
        "interpreter.invoke()\n",
        "predictions = interpreter.get_tensor(output_index)\n",
        "print(predictions)\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "plt.imshow(x_test[1])\n",
        "template = \"True:{true}, predicted:{predict}\"\n",
        "_ = plt.title(template.format(true= str(y_test[1]),\n",
        "                              predict=str(np.argmax(predictions[0]))))\n",
        "plt.grid(False)\n",
        "print(y_test[0:15])'''"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'input_index_quant = interpreter_quant.get_input_details()[0][\"index\"]\\noutput_index_quant = interpreter_quant.get_output_details()[0][\"index\"]\\n\\ntest_image = np.expand_dims(x_test[1], axis=0).astype(np.float32)\\n\\ninput_index = interpreter.get_input_details()[0][\"index\"]\\noutput_index = interpreter.get_output_details()[0][\"index\"]\\ninterpreter.set_tensor(input_index, test_image)\\ninterpreter.invoke()\\npredictions = interpreter.get_tensor(output_index)\\nprint(predictions)\\nimport matplotlib.pylab as plt\\n\\nplt.imshow(x_test[1])\\ntemplate = \"True:{true}, predicted:{predict}\"\\n_ = plt.title(template.format(true= str(y_test[1]),\\n                              predict=str(np.argmax(predictions[0]))))\\nplt.grid(False)\\nprint(y_test[0:15])'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJlv2aJCckcJ",
        "colab_type": "code",
        "outputId": "c78b2145-4e6d-44df-e6ba-c7268f4dc5ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# A helper function to evaluate the TF Lite model using \"test\" dataset.\n",
        "def evaluate_model(interpreter):\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on every image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  for test_image in x_test:\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  accurate_count = 0\n",
        "  for index in range(len(prediction_digits)):\n",
        "    if prediction_digits[index] == y_test[index]:\n",
        "      accurate_count += 1\n",
        "  accuracy = accurate_count * 1.0 / len(prediction_digits)\n",
        "\n",
        "  return accuracy\n",
        "\n",
        "print(evaluate_model(interpreter_1))            # float - 32\n",
        "print(evaluate_model(interpreter_2))            # float - 16\n",
        "print(evaluate_model(interpreter_3))            # hybrid\n",
        "print(evaluate_model(interpreter_4))            # int quant"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8814\n",
            "0.8812\n",
            "0.8814\n",
            "0.528\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND6d5BGrc2DR",
        "colab_type": "code",
        "outputId": "918c9444-a8af-42e4-8ef5-a7aca8ba635d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}